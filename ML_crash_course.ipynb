{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_crash_course.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNq/crpyHco+0zNMIj5YzhL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quicksilverri/statistics/blob/main/ML_crash_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google ML Crash Course Notes\n",
        "\n",
        "[Link to the Course](https://developers.google.com/machine-learning/crash-course/)"
      ],
      "metadata": {
        "id": "yg3eikfe6WRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we kinda need to measure loss somehow:  \n",
        "L2 Loss (or Squared Error) of a given example is **(observation - prediction)^2**  \n",
        "We kinda care about minimising loss for all the data, so L2 Loss = Σ(y' - observation)^2 \n",
        "\n",
        "**Mean Squared Error (MSE) = 1/N * Σ(y' - prediction(x))^2**"
      ],
      "metadata": {
        "id": "ZLuJ1LxF63CG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent** - https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent  \n",
        "A technique to minimize loss by computing the gradients of loss with respect to the model's parameters, conditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss.\n",
        "\n",
        "**Stochastic Gradient Descent** - GD, but we check on one piece of data.\n",
        "\n",
        "**Mini-batch Gradient Descent** - GD, but we check on a small (10-1000) batch of data. "
      ],
      "metadata": {
        "id": "xdntfO_g-7JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONVERGENCE**   \n",
        "Informally, often refers to a state reached during training in which training loss and validation loss change very little or not at all with each iteration after a certain number of iterations. *In other words, a model reaches convergence when additional training on the current data will not improve the model.*"
      ],
      "metadata": {
        "id": "RKErqvqa_2kt"
      }
    }
  ]
}